[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mybook",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "gpa_analysis.html",
    "href": "gpa_analysis.html",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "",
    "text": "3 Introduction\nData for this analysis can be found here\nThis section will be our first attempt at modeling the data, using classical (frequentist) style approaches. Even though this is thebayesianbandit, I like providing frequentist approaches as well to show the differences and similarities between normal statistical modeling (frequentist) and bayesian modeling.\nFinally, getting into the bayesian part of the analysis! As with our classical approach, we will layout the framework we will use for this section. We will be using the bayesian model (Bayes theorem) shown below.\n\\[P(H|\\theta) = \\frac{P(\\theta|H) P(H)}{P(\\theta)} \\tag{6.1}\\]\n\\[H = \\text{Our Hypothesis (prior)}\\] \\[\\theta = \\text{Our Data}\\]"
  },
  {
    "objectID": "gpa_analysis.html#data-viz",
    "href": "gpa_analysis.html#data-viz",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "4.1 Data Viz",
    "text": "4.1 Data Viz\nThe first visualization we will look at is comparing the distributions of gpa and study hours. We want to get a side by side look so we will use the subplots functionality from matplotlib. As well, we will plot the median of the two respective distributions to know where the 50% percentile lies. The following code produces Figure 4.1.\n\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\");\n\n#fig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\nFigure 4.1: Distributions of GPA and Study Hours\n\n\n\n\nFrom Figure 4.1, we see that GPA appears to be slightly left skewed with a couple of outliers to the left of the distribution. Study hours appears to be more right skewed with more outliers towards the right of the distribution. Median for GPA is 3.62 and 15 for study hours.\nSince we are working with two quantitative variables, we can use a scatterplot to view how linear their relationship (ie correlation) as well as view the where each gpa falls for a given amount of study hour. The below code produces Figure 4.2.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 4.2: Scatterplot of GPA by Study Hours\n\n\n\n\nFigure 4.2 shows that the linear relationship between study hours and GPA is not very strong. An exact score of .14 for the correlation indicates a weak linear relationship. Further more, like we already saw from the histograms, the data appears to be more grouped towards the beginning of the x axis (0-20ish hours).\nWe can attempt to adjust the distribution and correlation of the dataset by removing outliers. However, because the outlying dots appear to follow the trend of the data (more or less…) and we always like keeping as much data as possible, this is merely an optional step to fulfill our curiosity.\nWe will define an outlier as a point that falls outside of a box-plot range. Below is the function we will use to define and identify outliers.\n\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\nThe function calculates the IQR of the dataset and then returns a filtered dataframe based on whether a point passes the upper or lower bound of the box-plot. For example, below we see how the function identifies outliers using the GPA attribute.\n\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\nSimilarily, we can view the outliers for study hours below.\n\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n2\n3.930\n45.0\n\n\n7\n3.400\n40.0\n\n\n13\n3.830\n60.0\n\n\n51\n3.868\n40.0\n\n\n57\n3.125\n36.0\n\n\n77\n3.566\n40.0\n\n\n83\n3.850\n69.0\n\n\n89\n3.700\n45.0\n\n\n122\n3.750\n40.0\n\n\n125\n3.500\n49.0\n\n\n130\n3.825\n60.0\n\n\n135\n3.600\n40.0\n\n\n169\n3.830\n60.0\n\n\n\n\n\n\n\nFigure 4.3 below shows which points are labeled as outliers from our scatterplot.\n\n#Plot original scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\")\n\n#Show outliers by study hours (color red)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"study_hours\"), color='red')\n\n#Show outliers by gpa (color yellow)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"gpa\"), color='yellow');\n\n\n\n\nFigure 4.3: Scatterplot of GPA and Study Hours (highlighted outliers)\n\n\n\n\nAs already indicated, the red dots are outliers according to the “study hours” metric while yellow indicates outlier for “gpa”. While the red dots are technically “outliers”, they do sort of follow the general trend and direction of the data whereas the yellow dot does not. Let’s try removing the yellow dot to see how it changes the correlation of the data.\n\n#Remove outliers from dataset\ndf_filtered = df.query(\"gpa &gt; 2.6\")\n\nAfter removing the gpa outlier, the scatterplot and corresponding correlation are as follows.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df_filtered)\ncorr_coef = np.corrcoef(df_filtered.study_hours, df_filtered.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 4.4: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nWe see that removing the gpa outlier had a minimal effect on the correlation (actually slightly decreased) thus “weakening” the linear relationship of the variables. Perhaps we can try a non-linear transformation to aid our non-linear data.\nTo do this, we will perform a log transformation on both gpa and study hours (on our original dataset) and a polynomial transformation on study hours. Below is the result of our transformations.\n\n#Plot log transformed data\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.regplot(x='study_hours', y='gpa', data=np.log(df), ax=ax1)\nax1.set_xlabel(\"log(study_hours)\")\nax1.set_ylabel(\"log(gpa)\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax1.text(np.log(30), np.log(2.8), f\"Correlation: {corr_coef:.2f}\")\n\n#Plot polynomial transformed data\ndf_poly = df.assign(study_hours = lambda x: x.study_hours**2)\nsns.regplot(x='study_hours', y='gpa', data=df_poly, ax=ax2)\nax2.set_xlabel(\"study_hours^2\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax2.text(50**2, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 4.5: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nIn Figure 4.5, we see that our transformations really had not effect on the linear relationship. For the sake of getting on with the modeling, we will use our base dataset and interpret the model accordingly (barring that the other model assumptions hold)."
  },
  {
    "objectID": "gpa_analysis.html#linear-regression-framework",
    "href": "gpa_analysis.html#linear-regression-framework",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "5.1 Linear Regression Framework",
    "text": "5.1 Linear Regression Framework\nSince we are dealing with “linear” data, it would make sense to attempt to attempt to fit a linear model to the data. Therefore, we will be utilizing a simple linear regression (SLR) model to model the relationship between GPA and study hours (per week). Below is the equation we will use.\n\\[\ny = X\\beta + \\epsilon\n\\tag{5.1}\\] \\[\\epsilon \\sim N(\\mu, \\sigma^2)\\]\nEquation 5.1 models our response variable is equal to our matrix of explanatory variables (in this case, just one) multiplied (scaled) by a matrix of betas, plus errors (residuals) that are distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nIn linear algebra/matrix notation, it would look something like this.\n\\[\n\\begin{bmatrix}\ny_1 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1 \\\\\n... & ... \\\\\n1 & x_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n... \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\tag{5.2}\\]\nWe utilize ordinary least squares (OLS) to estimate our coefficients (\\(\\beta\\) values) by minimizing the residual sum of squares (RSS). Below are the equations we will use.\n\\[\nRSS = \\epsilon^2_1 + ... + \\epsilon^2_n\n\\tag{5.3}\\]\n\\[\n\\epsilon = y_i - \\hat{y_i}\n\\tag{5.4}\\]\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\tag{5.5}\\]\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\tag{5.6}\\]\nBy estimating our coefficients like this, we have a mathematical framework of estimating the line of best fit (the average line between all data points). This yields are final equation below.\n\\[\n\\hat{y} = X\\hat{\\beta}\n\\tag{5.7}\\]\nWhere \\(\\hat{y}\\) is our “predicted” value based off our inputs in the matrix of \\(X\\), scaled/multiplied by \\(\\hat{\\beta}\\), our estimated coefficients.\n\n5.1.1 Calculate beta values and interpret results\nNow that we have our framework by which we will fit our data, we can use Python libraries to perform the above calculations.\n\n#Import statsmod and fit SLR model\nimport statsmodels.api as sm\ny = df['gpa']\nX = pd.DataFrame({'intercept':np.ones(df.shape[0]), 'study_hours':df['study_hours']})\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n\n#Print summary from SLR model\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngpa\nR-squared:\n0.019\n\n\nModel:\nOLS\nAdj. R-squared:\n0.014\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.714\n\n\nDate:\nWed, 06 Sep 2023\nProb (F-statistic):\n0.0555\n\n\nTime:\n20:33:28\nLog-Likelihood:\n-27.443\n\n\nNo. Observations:\n193\nAIC:\n58.89\n\n\nDf Residuals:\n191\nBIC:\n65.41\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n3.5249\n0.037\n95.292\n0.000\n3.452\n3.598\n\n\nstudy_hours\n0.0034\n0.002\n1.927\n0.055\n-8.05e-05\n0.007\n\n\n\n\n\n\nOmnibus:\n10.671\nDurbin-Watson:\n2.385\n\n\nProb(Omnibus):\n0.005\nJarque-Bera (JB):\n11.481\n\n\nSkew:\n-0.595\nProb(JB):\n0.00321\n\n\nKurtosis:\n2.897\nCond. No.\n38.3\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFrom the summary, we have \\(\\beta_0\\) = 3.5249 and \\(\\beta_1\\) = .0034, meaning on average someone who studies 0 hours a week would see a GPA value of about 3.5. In addition, on average, for an increase of 1 hour in study time, we would see an increase of about .0034 in GPA. This isn’t too surprising considering the weak linear relationship we observed earlier in Figure 4.2. Essentially, if I were a student in this class, in order to change a whole letter grade (i.e. go from B to B+) and assuming I was right at B level (3.0), I would need to study 88 extra hours. Essentially, this model says that you’re stuck where you’re at.\nNevertheless, we can gain confidence in knowing a couple of things.\n\n\nThe data has a fairly weak linear relationship, so using a linear model to fit the data may not tell the most accurate story on how GPA is influenced by study hours.\n\n\nThe data could be inaccurate due to self-reporting"
  },
  {
    "objectID": "gpa_analysis.html#is-study-hours-a-statistically-significant-variable-in-determining-gpa",
    "href": "gpa_analysis.html#is-study-hours-a-statistically-significant-variable-in-determining-gpa",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "5.2 Is study hours a statistically significant variable in determining GPA?",
    "text": "5.2 Is study hours a statistically significant variable in determining GPA?\nTo determine the answer to the above question, we pose the following hypotheses.\n\\[\nH_0: \\beta_1 = 0\n\\] \\[\nH_a: \\beta_1 \\neq 0\n\\]\nThe aim of these hypotheses is to determine whether or not \\(\\beta_1\\) on average always has some kind of impact on GPA (whether it is an increase or decrease, no one knows in these things). We set our \\(\\alpha\\) = .05 to run our hypotheses.\nThe p-value we obtain from the summary print out indicates that p-value &gt; \\(\\alpha\\), so we fail to reject \\(H_0\\) and conclude that there isn’t sufficient evidence to reject the null. Essentially, we don’t have enough statistical evidence from the data to prove that study hours, on average, will never have a 0 impact on GPA.\nTo confirm this notion, we can view the corresponding 95% confidence interval for \\(\\beta_1\\) provided in the summary tab. The values of (-.000085, .007) indicate that 0 is included in the interval and therefore, the average change on GPA based on study hours can be 0."
  },
  {
    "objectID": "gpa_analysis.html#check-assumptions-of-linear-regression-model",
    "href": "gpa_analysis.html#check-assumptions-of-linear-regression-model",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "5.3 Check assumptions of linear regression model",
    "text": "5.3 Check assumptions of linear regression model\nEven though the \\(\\beta_1\\) value proved to not be statistically significant (though very close), we should still verify that the assumptions of our linear model hold with this data set. The assumptions for a linear regression model are listed below.\n\n\nLinearity\n\n\nIndependence\n\n\nNormality\n\n\nEqual Variance\n\n\nWe already verified that there is a weak linear relationship, but still a linear relationship since correlation was not 0. Independence is verified by the fact that each observation in the study was independent of one another, thus making them i.i.d random variables.\nFor the normality assumption, we can plot a histogram of the residuals to check that the residuals are approximately normally distributed. Below is the graph.\n\n#Plot histogram of residuals\nplt.figure(figsize=(15,5))\nsns.histplot(results.resid);\n\n\n\n\nFigure 5.1: Histogram of resiudals\n\n\n\n\nAs we can see in Figure 5.1, the distribution looks fairly left skewed with a long left tail. For ease of this analysis/exercise, we will say this is “approximately” normal, but this would more than likely fail in the real world.\nTo verify equal variance, we will view a scatterplot of the fitted vs residual values to make sure that there appears to be equal variance (good randomness, no trends) in the graph.\n\n#Plot scatterplot of fitted vs residuals\nplt.figure(figsize=(15,5))\nsns.scatterplot(x=results.fittedvalues,y=results.resid);\n\n\n\n\nFigure 5.2: Scatterplot of fitted vs residuals\n\n\n\n\nIn Figure 5.2, there appears to be no significant trends in the data (though it does tend to begin to taper as we increase across the x-axis). However, since there is nothing obvious or glaring, we can verify this assumption."
  },
  {
    "objectID": "gpa_analysis.html#do-students-spend-on-average-9-hours-a-week-studying",
    "href": "gpa_analysis.html#do-students-spend-on-average-9-hours-a-week-studying",
    "title": "2  Analysis of relationship between GPA and Study Hours",
    "section": "5.4 Do students spend on average 9 hours a week studying?",
    "text": "5.4 Do students spend on average 9 hours a week studying?\nFrom my past university experience, many professors have told me that for every 1 hour of lecture, should be 3 hours of study/homework time. Assuming this class was a 3 credit hour class, most students should be spending about 9 hours studying outside of class. To test this theory, we will use a t-test for the following hypotheses.\n\\[H_0: \\mu = 9\\] \\[H_a: \\mu \\neq 9\\]\n\n#Perform t-test to see if students on average spend 9 hours studying\nfrom scipy.stats import ttest_1samp\nttest_1samp(a=df['study_hours'], popmean=9)\n\nTtestResult(statistic=10.32185697364204, pvalue=3.767865100544389e-20, df=192)\n\n\nThe above result with a super small p-value (less than \\(\\alpha\\) = .05) indicates that we can reject the null hypothesis and conclude that students do not study an average of 9 hours for this class. To determine what the range of possible average values are, we will calculate a 95% confidence interval below.\n\n#Calculate 95% confidence interval\nttest_1samp(a=df['study_hours'], popmean=9).confidence_interval()\n\nConfidenceInterval(low=15.856880282374465, high=19.096487593273206)\n\n\nWe are 95% confident that the true average study time spent is between 15.85 to 19.09 hours. So, on average, students are spending a lot more time studying for this class than the hoped benchmark set by this university (assuming this university has set the same benchmark as my other ones)."
  }
]