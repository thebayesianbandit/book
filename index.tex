% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Classic and Bayesian Statistical Methods: An Introduction With Applications in Python},
  pdfauthor={Brandon Scott},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Classic and Bayesian Statistical Methods: An Introduction With
Applications in Python}
\author{Brandon Scott}
\date{2023-09-20}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, breakable, enhanced, frame hidden, sharp corners, borderline west={3pt}{0pt}{shadecolor}, interior hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{about-the-book}{%
\chapter*{About the book}\label{about-the-book}}
\addcontentsline{toc}{chapter}{About the book}

\markboth{About the book}{About the book}

Welcome to the first edition of this book!.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

Elements of Statistical Learning and Introduction to Statistical
Learning are two of the best books for becoming familiar with
statistical learning, the math methodologies behind them and practical
applications of said methodologies. These books are part of the
motivation as to why I chose to write Classic and Bayesian Statistical
Methods. There are an abundant of resources for learning statistical
theory and practical applications of said theory. However, there are few
books (if any that I know of) that cover both classic and bayesian
methodologies like how Elements of Statistical Learning and Introduction
to Statistical Learning illustrate statistical learning.

In Classic and Bayesian Statistical Methods, the goal is to educate the
reader of both approaches in both theory and application. The book
highlights the importance of understanding the mathematical background
of each approach and implementing those respective approaches in data
analysis situations. This book does not attempt to be an in-depth look
into the nuances of statistical theory. Rather, as the title suggests,
this is an introductory book to allow readers to gain an appreciation
and taste for both approaches, their usefulness, their weakness, and
overall a respect for statistical methods in general.

In short, the hope is that this book will be a tool for all
statisticians at any level. Whether you are an aspiring statistician in
your first year of undergrad, or a seasoned analyst looking to view new
ways of analyzing and exploring data, this book aims to be a solid
foundation of the possibilities in both classic and bayesian
methodologies so that you know how to best tackle your data problems.

\part{Chapters}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{statistics-the-liars-math}{%
\section{Statistics: The Liar's Math}\label{statistics-the-liars-math}}

Winston Churchill has been credited with saying ``The only statistics
you can trust are those you falsified yourself'' (Centre, n.d.). In many
instances, people attribute statistics (and statisticians alike) as
numbers twisted and manipulated to fit an agenda. To put it lightly, as
the section title suggests, people attribute statistics as a way for
liars to justify their conclusions using ``scientific'' means.

While there is valid proof that statistics have been weaponized in this
way in the past (and present, and more than likely in the future),
statistical methods are nothing more than tools. Used properly, and the
statistical methods can be edifying, enlightening, and useful in solving
problems. Used improperly, and you can have a dangerous tool, weaponized
by ignorance and stupidity. That is why understanding statistical
methods is so important. Statistics, as statistician Shane Reese puts
it, is ``decision making in the presence of uncertainty'' (Gardner
2023).

\hypertarget{inference-the-beauty-behind-the-numbers}{%
\section{Inference: The Beauty Behind the
Numbers}\label{inference-the-beauty-behind-the-numbers}}

One of the most important reasons why we perform statistical
calculations is to gather information and arrive at conclusions or
obtain answers to questions. Gathering data and observing the
relationships between various data points is the art of statistics, or
rather, statistical inference is the reason we perform statistical
calculations. Consider the realistic but hypothetical scenario where a
business is launching a new product and wants to know the most effective
way to advertise said product on their website. Collecting data on how
users interact with the website and the respective advertisements for
the new product allow us insights into the effectiveness of the ads,
thus giving us an edge on inferring how a random new website user might
interact with the advertisement and subsequently purchase the product.

All research questions in the end have one goal: to obtain valid
statistical inference on the collected data. Without it, many find it
hard to defend claims or push agendas on any solutions they propose.
Therefore, the goal of this book is to help readers understand how
statistical methodologies, both classic and bayesian, help people gather
statistical inference and consequently, arrive at valid conclusions that
drive meaningful impact in their respective careers/organizations.

\hypertarget{classic-statistical-methods}{%
\section{Classic Statistical
Methods}\label{classic-statistical-methods}}

Classic statistical methods should be the ones readers are most familiar
with (assuming a given reader has taken at least an introductory
statistics course). Classic statistical methods are the bread and butter
of modern day data analysis and data science. Simple linear regression,
analysis of variance, logistic regression, are all implementations of
classic statistical methods. These methods are, mathematically speaking,
simple to use and calculate, hence their rise in popularity (especially
before computers/computing power became widely available). Many may also
coin these methods as ``frequentist'' approaches, due to the philosophy
that their is a fixed parameter of interest and the goal is to estimate
said parameter from collected data from experiments/research.

An example of this would be examining the validity of a coin. You flip
the coin X number of times and from the collected data, make
conclusions/gather inference on whether or not it is a fair coin based
on your experiment. There are several famous examples of classic
statistical methods in the real world, such as Garden Pea Experiment by
Gregor Mendel. All of these demonstrate the effectiveness of collecting
data and performing mathematical calculations on data to arrive at valid
statistical conclusions.

\hypertarget{bayesian-statistical-methods}{%
\section{Bayesian Statistical
Methods}\label{bayesian-statistical-methods}}

Bayesian methods are becoming more popular in the data science age.
Bayesian methods take the philosophy that the parameter of interest is a
``random'' variable, contrasting the idea of a fixed parameter from the
classic approach. This provokes the idea that parameters of interest
derive from a distribution of values and that a parameter of interest
can have probabilites assigned to those distribution values.

An example of this philosophy would be a person waiting for the bus. A
person may have a notion of when the bus will arrive in some time frame
(10-15 minutes) from a given time stamp. This person will generally
believe that a bus will more than likely come sooner rather than later
based on the time they arrive at the station. This belief of uncertainty
and assigning probabilities to arrival times is the essence of bayesian
methods. Bayesian methods look for the distribution and uncertainy
behind parameters of interest.

\hypertarget{what-to-expect-from-the-book}{%
\section{What to expect from the
book}\label{what-to-expect-from-the-book}}

This book, as stated in the preface and sporatically in the
introduction, aims to inform readers on the usefulness of both classic
and bayesian statistical methods in various researchareas. Each chapter,
we will cover a popular topic/method in both statistical methods and
apply those methods to a dataset. The hope is that by working on a
dataset and illustrating the analytical flow of each method, readers
will gain a solid understanding of when to use different methods and how
to apply them to their own work.

We do not expect readers to become experts in any one of these proposed
statistical methods nor in any of the discussed topics in this book,
just from this book. Rather, as the book title suggests, this book is an
introduction to these methods and hopes to spark interest in the reader
to pursue further research in any of the methods and topics discussed in
this book. There are a plethora of resources available in the statistics
and data science community to further knowledge and expertise in any one
of the fields. Nonetheless, all learning begins with introductions and
basics so we hope that this book will prove to be just that for you as
you study this material.

\hypertarget{simple-linear-regression}{%
\chapter{Simple Linear Regression}\label{simple-linear-regression}}

The first method we will discuss is simple linear regression. Simple
linear regression is a staple of statistics and inference. It is one of
the most basic but still very useful methods at exploring data and
gathering inference between two variables of interest, particularly a
quantitative (continuous) response variable. While some may overlook the
importance of understanding simple linear regression, this chapter
serves to be a foundation for the rest of the book as subsequent
chapters will utilize the foundations established here.

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

In this chapter, we will utilize a dataset that aims to study the
effects of study hours on GPA. An annonymized private university in the
USA collected data from 193 students that self-reported their GPA and
respective amount of study time spent per week on the class (measured in
hours). There are several possible research questions that can arise
from the data, but the hope is to demonstrate that there is some kind of
valid quantifiable relationship between the amount a student studies in
the class and the GPA achieved.

Our hope would be that there is some sort of ``linear'' relationship
between study hours and GPA. That is, for a increase in study time,
there is generally an increase in GPA. Later in this chapter, we will
establish the mathematical framework by which we can (hopefully) prove
such a relationship. For now, exploration of the data is the first
important step in our analytical flow as we would like to know what the
data looks like.

Data for this analysis can be found
\href{https://www.kaggle.com/datasets/joebeachcapital/gpa-study-hours}{here}

\hypertarget{exploratory-data-analysis-eda}{%
\section{Exploratory Data Analysis
(EDA)}\label{exploratory-data-analysis-eda}}

Exploratory data analysis (EDA) is an important step in utilizing
statistical methods since without it, we wouldn't know what kind of
methods to apply to the data at hand! By utilizing various visualization
libraries and numerical summaries, we can properly identify trends in
the data and correctly apply proper statistical methodologies to achieve
valid results. To begin, we import various libraries that will be useful
for our analysis such as pandas, numpy, matplotlib, and seaborn.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Import libraries}
\ImportTok{import}\NormalTok{ opendatasets }\ImportTok{as}\NormalTok{ od}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\end{Highlighting}
\end{Shaded}

We begin our analysis by viewing reading our data into a pandas
dataframe and viewing the first 5 observations by using the head method
of the dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Check dataframe}
\NormalTok{df.head().style.}\BuiltInTok{format}\NormalTok{(precision}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\caption{}\label{T_832fa}\tabularnewline
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 4.000 & 10.000 \\
1 & 3.800 & 25.000 \\
2 & 3.930 & 45.000 \\
3 & 3.400 & 10.000 \\
4 & 3.200 & 4.000 \\
\end{longtable}

The head result shows that we have two variables organized into columns
(gpa and study\_hours). Since it would be hard to look at all
observations of the data at once in their tabular form to identify
trends, we will use the describe method to see basic summary statistics
on the data. This is shown in Table~\ref{tbl-describe}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.describe().style.}\BuiltInTok{format}\NormalTok{(precision}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-describe}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-describe}Descriptive Summary
Statistics}\label{T_2a29d}\tabularnewline
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
count & 193.000 & 193.000 \\
mean & 3.586 & 17.477 \\
std & 0.285 & 11.409 \\
min & 2.600 & 2.000 \\
25\% & 3.400 & 10.000 \\
50\% & 3.620 & 15.000 \\
75\% & 3.800 & 20.000 \\
max & 4.300 & 69.000 \\
\end{longtable}

The resulting dataframe shows that we have 193 rows of data where on
average the gpa is 3.58 with a standard deviation of .28. Average study
hours show 17.47 and standard deviation of 11.4, meaning we have a large
spread in the distribution of study hours but not too much in gpa. From
the summary statistics, we can easily see that the max value is invalid
as the scale we are using for this analysis is 0-4. We will eliminate
any rows that fall outside of this range.

To do this, we will utilize the assign method in our dataframe and pass
a lambda function using numpy's where method to filter out any gpa that
is above the threshold of 4.0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Convert outlier GPA (erroneous entries) to 4.0}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.assign(gpa }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: np.where(x.gpa }\OperatorTok{\textgreater{}} \DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, x.gpa))}
\end{Highlighting}
\end{Shaded}

Now that we have cleaned up our data a bit and have viewed some of the
high-level numerical summaries of the data, we want to begin viewing the
data to identify trends and understand the distribution of the data we
are working with.

The first visualization we will look at is comparing the distributions
of gpa and study hours. We want to get a side by side look so we will
use the subplots functionality from matplotlib. As well, we will plot
the median of the two respective distributions to know where the 50\%
percentile lies. The following code produces Figure~\ref{fig-dist-1}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check distributios of gpa and study hours}
\NormalTok{fig, (ax1, ax2) }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize }\OperatorTok{=}\NormalTok{ (}\DecValTok{18}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{sns.histplot(x}\OperatorTok{=}\StringTok{"gpa"}\NormalTok{, data}\OperatorTok{=}\NormalTok{df, ax}\OperatorTok{=}\NormalTok{ax1, bins}\OperatorTok{=}\DecValTok{15}\NormalTok{)}
\NormalTok{ax1.axvline(df.gpa.median(), color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dashed"}\NormalTok{)}
\NormalTok{ax1.text(df.gpa.median() }\OperatorTok{{-}} \FloatTok{0.3}\NormalTok{, ax1.get\_ylim()[}\DecValTok{1}\NormalTok{] }\OperatorTok{*} \FloatTok{0.9}\NormalTok{, }\SpecialStringTok{f"Median: }\SpecialCharTok{\{}\NormalTok{df}\SpecialCharTok{.}\NormalTok{gpa}\SpecialCharTok{.}\NormalTok{median()}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{sns.histplot(x}\OperatorTok{=}\StringTok{"study\_hours"}\NormalTok{, data}\OperatorTok{=}\NormalTok{df, ax}\OperatorTok{=}\NormalTok{ax2, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, bins}\OperatorTok{=}\DecValTok{15}\NormalTok{)}
\NormalTok{ax2.axvline(df.study\_hours.median(), color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dashed"}\NormalTok{)}
\NormalTok{ax2.text(df.study\_hours.median() }\OperatorTok{+} \DecValTok{2}\NormalTok{, ax2.get\_ylim()[}\DecValTok{1}\NormalTok{] }\OperatorTok{*} \FloatTok{0.9}\NormalTok{, }\SpecialStringTok{f"Median: }\SpecialCharTok{\{}\NormalTok{df}\SpecialCharTok{.}\NormalTok{study\_hours}\SpecialCharTok{.}\NormalTok{median()}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}\OperatorTok{;}

\CommentTok{\#fig.suptitle("Distributions of GPA and Study Hours");}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-dist-1-output-1.png}

}

\caption{\label{fig-dist-1}Distributions of GPA and Study Hours}

\end{figure}

From Figure~\ref{fig-dist-1}, we see that GPA appears to be slightly
left skewed with a couple of outliers to the left of the distribution.
Study hours appears to be more right skewed with more outliers towards
the right of the distribution. Median for GPA is 3.62 and 15 for study
hours. Additionally, study hours appears to be bimodal with a large gap
between the median and 20 hour study mark.

Since we are working with two quantitative variables, we can use a
scatterplot to view how linear their relationship (ie correlation) as
well as view the where each gpa falls for a given amount of study hour.
The below code produces Figure~\ref{fig-scatterplot-1}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Scatterplot of gpa and study hours}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df)}
\NormalTok{corr\_coef }\OperatorTok{=}\NormalTok{ np.corrcoef(df.study\_hours, df.gpa)[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\NormalTok{plt.text(}\DecValTok{50}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\SpecialStringTok{f"Correlation: }\SpecialCharTok{\{}\NormalTok{corr\_coef}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-scatterplot-1-output-1.png}

}

\caption{\label{fig-scatterplot-1}Scatterplot of GPA by Study Hours}

\end{figure}

Figure~\ref{fig-scatterplot-1} shows that the linear relationship
between study hours and GPA is not very strong. An exact score of .14
for the correlation indicates a weak linear relationship. Further more,
like we already saw from the histograms, the data appears to be more
grouped towards the beginning of the x axis (0-20ish hours).

We can attempt to adjust the distribution and correlation of the dataset
by removing outliers. However, because the outlying dots appear to
follow the trend of the data (more or less\ldots) and we always like
keeping as much data as possible, this is merely an optional step to
fulfill our curiosity.

We will define an outlier as a point that falls outside of a box-plot
range. Below is the function we will use to define and identify
outliers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Identify outliers}
\KeywordTok{def}\NormalTok{ find\_outliers(x, column\_name):}
\NormalTok{    q1 }\OperatorTok{=}\NormalTok{ x[column\_name].quantile(}\FloatTok{.25}\NormalTok{)}
\NormalTok{    q3 }\OperatorTok{=}\NormalTok{ x[column\_name].quantile(}\FloatTok{.75}\NormalTok{)}
\NormalTok{    iqr }\OperatorTok{=}\NormalTok{ q3 }\OperatorTok{{-}}\NormalTok{ q1}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ q1 }\OperatorTok{{-}}\NormalTok{ (}\FloatTok{1.5} \OperatorTok{*}\NormalTok{ iqr)}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ q3 }\OperatorTok{+}\NormalTok{ (}\FloatTok{1.5} \OperatorTok{*}\NormalTok{ iqr)}
\NormalTok{    outliers }\OperatorTok{=}\NormalTok{ x[(x[column\_name] }\OperatorTok{\textless{}}\NormalTok{ lower) }\OperatorTok{|}\NormalTok{ (x[column\_name] }\OperatorTok{\textgreater{}}\NormalTok{ upper)]}
    
    \ControlFlowTok{return}\NormalTok{ outliers}
    
\end{Highlighting}
\end{Shaded}

The function calculates the IQR of the dataset and then returns a
filtered dataframe based on whether a point passes the upper or lower
bound of the box-plot. For example, below we see how the function
identifies outliers using the GPA attribute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Find outliers for gpa}
\NormalTok{find\_outliers(df, }\StringTok{"gpa"}\NormalTok{).style.}\BuiltInTok{format}\NormalTok{(precision}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\caption{}\label{T_2e3cc}\tabularnewline
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
108 & 2.600 & 7.000 \\
\end{longtable}

Similarily, we can view the outliers for study hours below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Find outliers for study hours}
\NormalTok{find\_outliers(df, }\StringTok{"study\_hours"}\NormalTok{).style.}\BuiltInTok{format}\NormalTok{(precision}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\caption{}\label{T_7c3b2}\tabularnewline
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
~ & gpa & study\_hours \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & 3.930 & 45.000 \\
7 & 3.400 & 40.000 \\
13 & 3.830 & 60.000 \\
51 & 3.868 & 40.000 \\
57 & 3.125 & 36.000 \\
77 & 3.566 & 40.000 \\
83 & 3.850 & 69.000 \\
89 & 3.700 & 45.000 \\
122 & 3.750 & 40.000 \\
125 & 3.500 & 49.000 \\
130 & 3.825 & 60.000 \\
135 & 3.600 & 40.000 \\
169 & 3.830 & 60.000 \\
\end{longtable}

Figure~\ref{fig-scatter-outliers} below shows which points are labeled
as outliers from our scatterplot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plot original scatterplot}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df)}
\NormalTok{corr\_coef }\OperatorTok{=}\NormalTok{ np.corrcoef(df.study\_hours, df.gpa)[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\NormalTok{plt.text(}\DecValTok{50}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\SpecialStringTok{f"Correlation: }\SpecialCharTok{\{}\NormalTok{corr\_coef}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\#Show outliers by study hours (color red)}
\NormalTok{sns.scatterplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{find\_outliers(df, }\StringTok{"study\_hours"}\NormalTok{), color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}

\CommentTok{\#Show outliers by gpa (color yellow)}
\NormalTok{sns.scatterplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{find\_outliers(df, }\StringTok{"gpa"}\NormalTok{), color}\OperatorTok{=}\StringTok{\textquotesingle{}yellow\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-scatter-outliers-output-1.png}

}

\caption{\label{fig-scatter-outliers}Scatterplot of GPA and Study Hours
(highlighted outliers)}

\end{figure}

As already indicated, the red dots are outliers according to the ``study
hours'' metric while yellow indicates outlier for ``gpa''. While the red
dots are technically ``outliers'', they do sort of follow the general
trend and direction of the data whereas the yellow dot does not. Let's
try removing the yellow dot to see how it changes the correlation of the
data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Remove outliers from dataset}
\NormalTok{df\_filtered }\OperatorTok{=}\NormalTok{ df.query(}\StringTok{"gpa \textgreater{} 2.6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After removing the gpa outlier, the scatterplot and corresponding
correlation are as follows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Scatterplot of gpa and study hours}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df\_filtered)}
\NormalTok{corr\_coef }\OperatorTok{=}\NormalTok{ np.corrcoef(df\_filtered.study\_hours, df\_filtered.gpa)[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\NormalTok{plt.text(}\DecValTok{50}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\SpecialStringTok{f"Correlation: }\SpecialCharTok{\{}\NormalTok{corr\_coef}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-scatterplot-2-output-1.png}

}

\caption{\label{fig-scatterplot-2}Scatterplot of GPA by Study Hours
(filtered)}

\end{figure}

We see that removing the gpa outlier had a minimal effect on the
correlation (actually slightly decreased) thus ``weakening'' the linear
relationship of the variables. Perhaps we can try a non-linear
transformation to aid our non-linear data.

To do this, we will perform a log transformation on both gpa and study
hours (on our original dataset) and a polynomial transformation on study
hours. Below is the result of our transformations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plot log transformed data}
\NormalTok{fig, (ax1, ax2) }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize }\OperatorTok{=}\NormalTok{ (}\DecValTok{18}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{np.log(df), ax}\OperatorTok{=}\NormalTok{ax1)}
\NormalTok{ax1.set\_xlabel(}\StringTok{"log(study\_hours)"}\NormalTok{)}
\NormalTok{ax1.set\_ylabel(}\StringTok{"log(gpa)"}\NormalTok{)}
\NormalTok{corr\_coef }\OperatorTok{=}\NormalTok{ np.corrcoef(df.study\_hours, df.gpa)[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\NormalTok{ax1.text(np.log(}\DecValTok{30}\NormalTok{), np.log(}\FloatTok{2.8}\NormalTok{), }\SpecialStringTok{f"Correlation: }\SpecialCharTok{\{}\NormalTok{corr\_coef}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\#Plot polynomial transformed data}
\NormalTok{df\_poly }\OperatorTok{=}\NormalTok{ df.assign(study\_hours }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: x.study\_hours}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df\_poly, ax}\OperatorTok{=}\NormalTok{ax2)}
\NormalTok{ax2.set\_xlabel(}\StringTok{"study\_hours\^{}2"}\NormalTok{)}
\NormalTok{corr\_coef }\OperatorTok{=}\NormalTok{ np.corrcoef(df.study\_hours, df.gpa)[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\NormalTok{ax2.text(}\DecValTok{50}\OperatorTok{**}\DecValTok{2}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\SpecialStringTok{f"Correlation: }\SpecialCharTok{\{}\NormalTok{corr\_coef}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-transformations-output-1.png}

}

\caption{\label{fig-transformations}Scatterplot of GPA by Study Hours
(filtered)}

\end{figure}

In Figure~\ref{fig-transformations}, we see that our transformations
really had not effect on the linear relationship. For the sake of
getting on with the modeling, we will use our base dataset and interpret
the model accordingly (barring that the other model assumptions hold).

\hypertarget{classical-modeling}{%
\section{Classical Modeling}\label{classical-modeling}}

While the EDA above did not demonstrate a strong linear relationship
between study hours and GPA, it did show that there was at least some
kind of linear relationship between the two variables. Therefore, a
simple linear regression model can still work in this situation.

\hypertarget{linear-regression-framework}{%
\subsection{Linear Regression
Framework}\label{linear-regression-framework}}

A simple linear regression model is a simple model that measures the
effect of a single predictor on a single quantitative response variable.
In our analysis, we wish to know the effect study hours have of GPA.
Since we wish to demonstrate that this relationship is indeed linear, we
will utilize a linear model found in Equation~\ref{eq-linear-model}.

\begin{equation}\protect\hypertarget{eq-linear-model}{}{
Y = \beta_0 + \beta_1X
}\label{eq-linear-model}\end{equation}

Equation~\ref{eq-linear-model} is the simple linear model we all should
have learned back in algebra. \(Y\) in this situation is our response
variable (GPA) being modeled linearly by a predictor \(X\) (study hours)
multiplied by some slope \(\beta_1\) and incremented by the y intercept
of the line at \(\beta_0\).

Our formula in Equation~\ref{eq-linear-model} gives us a good starting
point on how we can properly quantify the relationship between study
hours and GPA. We currently do not know the values of \(\beta_0\) and
\(\beta_1\). To produce these values, we will use a method called
ordinary least squares (OLS) to estimate these beta values.

OLS aims to find a line that minimizes the distance between all points
and said line. This calculated distances between a given line and the
data points are called residuals (see Equation~\ref{eq-residuals}). To
have a more mathematically feasible way of measuring the distance
between the points and a line, we square the residuals and sum them up,
which is shown in Equation~\ref{eq-residual-sum-squares}.

\begin{equation}\protect\hypertarget{eq-residual-sum-squares}{}{
RSS = \epsilon^2_1 + ... + \epsilon^2_n
}\label{eq-residual-sum-squares}\end{equation}

\begin{equation}\protect\hypertarget{eq-residuals}{}{
\epsilon = y_i - \hat{y_i}
}\label{eq-residuals}\end{equation}

We wish to find a line that minimizes RSS. In calculus, finding minimums
of functions is found by taking the derivative of the function we wish
to minimize. In this case, OLS finds the values of \(\beta_0\) and
\(\beta_1\) that minimizes RSS. Equation~\ref{eq-beta-1} and
Equation~\ref{eq-beta-0} show the resulting minimized functions.

\begin{equation}\protect\hypertarget{eq-beta-1}{}{
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
}\label{eq-beta-1}\end{equation}

\begin{equation}\protect\hypertarget{eq-beta-0}{}{
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
}\label{eq-beta-0}\end{equation}

Essentially, OLS produces the line that runs through the average of the
data (ie the point (\(\bar{x}\), \(\bar{y}\))). This is the line that
minimizes the Equation~\ref{eq-residual-sum-squares}. Because of this
line and our newly estimated beta values, we can propose our predictive
equation found in Equation~\ref{eq-linear-hat}

\begin{equation}\protect\hypertarget{eq-linear-hat}{}{
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
}\label{eq-linear-hat}\end{equation}

Equation~\ref{eq-linear-hat} is our predictive equation since we are
using our estimated betas to estimate our response \(y\) based on some
predictor value \(x\). It is important to recognize the difference
between Equation~\ref{eq-linear-hat} and Equation~\ref{eq-linear-model}
as our predictive one is based on our estimations and
Equation~\ref{eq-linear-model} is based on our theoretical application.

\hypertarget{calculate-beta-values-and-interpret-results}{%
\subsection{Calculate beta values and interpret
results}\label{calculate-beta-values-and-interpret-results}}

Now that we have our framework by which we will fit our data, we can use
Python libraries to perform the above calculations. The beta values from
our model are found in Table~\ref{tbl-beta-estimates}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Import statsmod and fit SLR model}
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\NormalTok{y }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}gpa\textquotesingle{}}\NormalTok{]}
\NormalTok{X }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}intercept\textquotesingle{}}\NormalTok{:np.ones(df.shape[}\DecValTok{0}\NormalTok{]), }\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{:df[}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{]\})}
\NormalTok{model }\OperatorTok{=}\NormalTok{ sm.OLS(y, X)}
\NormalTok{results }\OperatorTok{=}\NormalTok{ model.fit()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-beta-estimates}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-beta-estimates}Beta Estimates from
OLS}\tabularnewline
\toprule\noalign{}
Name & Beta Est. & p-value \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Name & Beta Est. & p-value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
intercept & 3.528 & .000 \\
study\_hours & .0033 & .065 \\
\end{longtable}

From the summary, we have \(\beta_0\) = 3.528 and \(\beta_1\) = .0033,
meaning on average someone who studies 0 hours a week would see a GPA
value of about 3.5. In addition, on average, for an increase of 1 hour
in study time, we would see an increase of about .0034 in GPA. This
isn't too surprising considering the weak linear relationship we
observed earlier in Figure~\ref{fig-scatterplot-1}. Essentially, if I
were a student in this class, in order to change a whole letter grade
(i.e.~go from B to B+) and assuming I was right at B level (3.0), I
would need to study 88 extra hours. Essentially, this model says that
you're stuck where you're at.

These estimates in Table~\ref{tbl-beta-estimates} illustrate the power
but also weaknesses and limitations of simple linear regression. While
we have quantified a relationship between GPA and study hours, because
of the weak linear relationship between these two variables, the average
increase in GPA for a 1 unit increase in study hours is very very small.

\hypertarget{testing-for-statistically-significant-relationships}{%
\subsection{Testing for statistically significant
relationships}\label{testing-for-statistically-significant-relationships}}

While the beta values shown in Table~\ref{tbl-beta-estimates} are the
results produced from our collected data, the question is how much can
we trust the values produced? As we illustrated in
Equation~\ref{eq-linear-model}, we have a theoretical model where some
values of beta equal our response Y. While in theory this may be
possible, in the real world, variability occurs and we must account for
that in the model. Equation~\ref{eq-linear-variability} demonstrates how
we account for this variability.

\[
Y = \beta_0 + \beta_1X + \epsilon
\] \begin{equation}\protect\hypertarget{eq-linear-variability}{}{
\epsilon \sim \mathcal{N}(\mu=0, \sigma^2)
}\label{eq-linear-variability}\end{equation}

Equation~\ref{eq-linear-variability} shows that there is some function
that models our response Y, but that there is some kind of variation
(randomness) that affect our betas, which need to be corrected by
\(\epsilon\). Essentially, \(\epsilon\) acts as our ``fix all'' solution
to our model that accounts for the variability in the world.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\# PLACE HOLDER FOR WRITING}
\end{Highlighting}
\end{Shaded}

\[
H_0: \beta_1 = 0
\] \[
H_a: \beta_1 \neq 0
\]

The aim of these hypotheses is to determine whether or not \(\beta_1\)
on average always has some kind of impact on GPA (whether it is an
increase or decrease, no one knows in these things). We set our
\(\alpha\) = .05 to run our hypotheses.

The p-value we obtain from the summary print out indicates that p-value
\textgreater{} \(\alpha\), so we fail to reject \(H_0\) and conclude
that there isn't sufficient evidence to reject the null. Essentially, we
don't have enough statistical evidence from the data to prove that study
hours, on average, will never have a 0 impact on GPA.

To confirm this notion, we can view the corresponding 95\% confidence
interval for \(\beta_1\) provided in the summary tab. The values of
(-.000085, .007) indicate that 0 is included in the interval and
therefore, the average change on GPA based on study hours can be 0.

\hypertarget{check-assumptions-of-linear-regression-model}{%
\subsection{Check assumptions of linear regression
model}\label{check-assumptions-of-linear-regression-model}}

Even though the \(\beta_1\) value proved to not be statistically
significant (though very close), we should still verify that the
assumptions of our linear model hold with this data set. The assumptions
for a linear regression model are listed below.

Linearity

Independence

Normality

Equal Variance

We already verified that there is a weak linear relationship, but still
a linear relationship since correlation was not 0. Independence is
verified by the fact that each observation in the study was independent
of one another, thus making them i.i.d random variables.

For the normality assumption, we can plot a histogram of the residuals
to check that the residuals are approximately normally distributed.
Below is the graph.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plot histogram of residuals}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{sns.histplot(results.resid)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-normality-output-1.png}

}

\caption{\label{fig-normality}Histogram of resiudals}

\end{figure}

As we can see in Figure~\ref{fig-normality}, the distribution looks
fairly left skewed with a long left tail. For ease of this
analysis/exercise, we will say this is ``approximately'' normal, but
this would more than likely fail in the real world.

To verify equal variance, we will view a scatterplot of the fitted vs
residual values to make sure that there appears to be equal variance
(good randomness, no trends) in the graph.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plot scatterplot of fitted vs residuals}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{15}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{sns.scatterplot(x}\OperatorTok{=}\NormalTok{results.fittedvalues,y}\OperatorTok{=}\NormalTok{results.resid)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gpa_analysis_files/figure-pdf/fig-equal-variance-output-1.png}

}

\caption{\label{fig-equal-variance}Scatterplot of fitted vs residuals}

\end{figure}

In Figure~\ref{fig-equal-variance}, there appears to be no significant
trends in the data (though it does tend to begin to taper as we increase
across the x-axis). However, since there is nothing obvious or glaring,
we can verify this assumption.

\hypertarget{do-students-spend-on-average-9-hours-a-week-studying}{%
\subsection{Do students spend on average 9 hours a week
studying?}\label{do-students-spend-on-average-9-hours-a-week-studying}}

From my past university experience, many professors have told me that
for every 1 hour of lecture, should be 3 hours of study/homework time.
Assuming this class was a 3 credit hour class, most students should be
spending about 9 hours studying outside of class. To test this theory,
we will use a t-test for the following hypotheses.

\[H_0: \mu = 9\] \[H_a: \mu \neq 9\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Perform t{-}test to see if students on average spend 9 hours studying}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ ttest\_1samp}
\NormalTok{ttest\_1samp(a}\OperatorTok{=}\NormalTok{df[}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{], popmean}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
TtestResult(statistic=10.32185697364204, pvalue=3.767865100544389e-20, df=192)
\end{verbatim}

The above result with a super small p-value (less than \(\alpha\) = .05)
indicates that we can reject the null hypothesis and conclude that
students do not study an average of 9 hours for this class. To determine
what the range of possible average values are, we will calculate a 95\%
confidence interval below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Calculate 95\% confidence interval}
\NormalTok{ttest\_1samp(a}\OperatorTok{=}\NormalTok{df[}\StringTok{\textquotesingle{}study\_hours\textquotesingle{}}\NormalTok{], popmean}\OperatorTok{=}\DecValTok{9}\NormalTok{).confidence\_interval()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ConfidenceInterval(low=15.856880282374465, high=19.096487593273206)
\end{verbatim}

We are 95\% confident that the true average study time spent is between
15.85 to 19.09 hours. So, on average, students are spending a lot more
time studying for this class than the hoped benchmark set by this
university (assuming this university has set the same benchmark as my
other ones).

\hypertarget{bayesian-modeling}{%
\section{Bayesian Modeling}\label{bayesian-modeling}}

Finally, getting into the bayesian part of the analysis! As with our
classical approach, we will layout the framework we will use for this
section. We will be using the bayesian model (Bayes theorem) shown
below.

\begin{equation}\protect\hypertarget{eq-bayes-theorem}{}{P(H|\theta) = \frac{P(\theta|H) P(H)}{P(\theta)}}\label{eq-bayes-theorem}\end{equation}

\[H = \text{Our Hypothesis (prior)}\] \[\theta = \text{Our Data}\]

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

In summary, this book has no content whatsoever.

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-church1874}{}}%
Centre, Churchill. n.d.
\url{https://www.causeweb.org/cause/resources/library/r609\#:~:text=The\%20only\%20statistics\%20you\%20can,Churchill\%20(1874\%20\%2D\%201965).}

\leavevmode\vadjust pre{\hypertarget{ref-byu2023}{}}%
Gardner, Peter B. 2023. {``Meet Shane Reese.''}
\url{https://magazine.byu.edu/article/outlier/}.

\end{CSLReferences}



\end{document}
