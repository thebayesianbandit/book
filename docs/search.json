[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classic and Bayesian Statistical Methods: An Introduction With Applications in Python",
    "section": "",
    "text": "About the book\nWelcome to the first edition of this book!.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Elements of Statistical Learning and Introduction to Statistical Learning are two of the best books for becoming familiar with statistical learning, the math methodologies behind them and practical applications of said methodologies. These books are part of the motivation as to why I chose to write Classic and Bayesian Statistical Methods. There are an abundant of resources for learning statistical theory and practical applications of said theory. However, there are few books (if any that I know of) that cover both classic and bayesian methodologies like how Elements of Statistical Learning and Introduction to Statistical Learning illustrate statistical learning.\nIn Classic and Bayesian Statistical Methods, the goal is to educate the reader of both approaches in both theory and application. The book highlights the importance of understanding the mathematical background of each approach and implementing those respective approaches in data analysis situations. This book does not attempt to be an in-depth look into the nuances of statistical theory. Rather, as the title suggests, this is an introductory book to allow readers to gain an appreciation and taste for both approaches, their usefulness, their weakness, and overall a respect for statistical methods in general.\nIn short, the hope is that this book will be a tool for all statisticians at any level. Whether you are an aspiring statistician in your first year of undergrad, or a seasoned analyst looking to view new ways of analyzing and exploring data, this book aims to be a solid foundation of the possibilities in both classic and bayesian methodologies so that you know how to best tackle your data problems."
  },
  {
    "objectID": "intro_stats.html#statistics-the-liars-math",
    "href": "intro_stats.html#statistics-the-liars-math",
    "title": "1  Introduction",
    "section": "Statistics: The Liar’s Math",
    "text": "Statistics: The Liar’s Math\nWinston Churchill has been credited with saying “The only statistics you can trust are those you falsified yourself” (Centre, n.d.). In many instances, people attribute statistics (and statisticians alike) as numbers twisted and manipulated to fit an agenda. To put it lightly, as the section title suggests, people attribute statistics as a way for liars to justify their conclusions using “scientific” means.\nWhile there is valid proof that statistics have been weaponized in this way in the past (and present, and more than likely in the future), statistical methods are nothing more than tools. Used properly, and the statistical methods can be edifying, enlightening, and useful in solving problems. Used improperly, and you can have a dangerous tool, weaponized by ignorance and stupidity. That is why understanding statistical methods is so important. Statistics, as statistician Shane Reese puts it, is “decision making in the presence of uncertainty” (Gardner 2023)."
  },
  {
    "objectID": "intro_stats.html#inference-the-beauty-behind-the-numbers",
    "href": "intro_stats.html#inference-the-beauty-behind-the-numbers",
    "title": "1  Introduction",
    "section": "Inference: The Beauty Behind the Numbers",
    "text": "Inference: The Beauty Behind the Numbers\nOne of the most important reasons why we perform statistical calculations is to gather information and arrive at conclusions or obtain answers to questions. Gathering data and observing the relationships between various data points is the art of statistics, or rather, statistical inference is the reason we perform statistical calculations. Consider the realistic but hypothetical scenario where a business is launching a new product and wants to know the most effective way to advertise said product on their website. Collecting data on how users interact with the website and the respective advertisements for the new product allow us insights into the effectiveness of the ads, thus giving us an edge on inferring how a random new website user might interact with the advertisement and subsequently purchase the product.\nAll research questions in the end have one goal: to obtain valid statistical inference on the collected data. Without it, many find it hard to defend claims or push agendas on any solutions they propose. Therefore, the goal of this book is to help readers understand how statistical methodologies, both classic and bayesian, help people gather statistical inference and consequently, arrive at valid conclusions that drive meaningful impact in their respective careers/organizations."
  },
  {
    "objectID": "intro_stats.html#classic-statistical-methods",
    "href": "intro_stats.html#classic-statistical-methods",
    "title": "1  Introduction",
    "section": "Classic Statistical Methods",
    "text": "Classic Statistical Methods\nClassic statistical methods should be the ones readers are most familiar with (assuming a given reader has taken at least an introductory statistics course). Classic statistical methods are the bread and butter of modern day data analysis and data science. Simple linear regression, analysis of variance, logistic regression, are all implementations of classic statistical methods. These methods are, mathematically speaking, simple to use and calculate, hence their rise in popularity (especially before computers/computing power became widely available). Many may also coin these methods as “frequentist” approaches, due to the philosophy that their is a fixed parameter of interest and the goal is to estimate said parameter from collected data from experiments/research.\nAn example of this would be examining the validity of a coin. You flip the coin X number of times and from the collected data, make conclusions/gather inference on whether or not it is a fair coin based on your experiment. There are several famous examples of classic statistical methods in the real world, such as Garden Pea Experiment by Gregor Mendel. All of these demonstrate the effectiveness of collecting data and performing mathematical calculations on data to arrive at valid statistical conclusions."
  },
  {
    "objectID": "intro_stats.html#bayesian-statistical-methods",
    "href": "intro_stats.html#bayesian-statistical-methods",
    "title": "1  Introduction",
    "section": "Bayesian Statistical Methods",
    "text": "Bayesian Statistical Methods\nBayesian methods are becoming more popular in the data science age. Bayesian methods take the philosophy that the parameter of interest is a “random” variable, contrasting the idea of a fixed parameter from the classic approach. This provokes the idea that parameters of interest derive from a distribution of values and that a parameter of interest can have probabilites assigned to those distribution values.\nAn example of this philosophy would be a person waiting for the bus. A person may have a notion of when the bus will arrive in some time frame (10-15 minutes) from a given time stamp. This person will generally believe that a bus will more than likely come sooner rather than later based on the time they arrive at the station. This belief of uncertainty and assigning probabilities to arrival times is the essence of bayesian methods. Bayesian methods look for the distribution and uncertainy behind parameters of interest."
  },
  {
    "objectID": "intro_stats.html#what-to-expect-from-the-book",
    "href": "intro_stats.html#what-to-expect-from-the-book",
    "title": "1  Introduction",
    "section": "What to expect from the book",
    "text": "What to expect from the book\nThis book, as stated in the preface and sporatically in the introduction, aims to inform readers on the usefulness of both classic and bayesian statistical methods in various researchareas. Each chapter, we will cover a popular topic/method in both statistical methods and apply those methods to a dataset. The hope is that by working on a dataset and illustrating the analytical flow of each method, readers will gain a solid understanding of when to use different methods and how to apply them to their own work.\nWe do not expect readers to become experts in any one of these proposed statistical methods nor in any of the discussed topics in this book, just from this book. Rather, as the book title suggests, this book is an introduction to these methods and hopes to spark interest in the reader to pursue further research in any of the methods and topics discussed in this book. There are a plethora of resources available in the statistics and data science community to further knowledge and expertise in any one of the fields. Nonetheless, all learning begins with introductions and basics so we hope that this book will prove to be just that for you as you study this material.\n\n\n\n\nCentre, Churchill. n.d. https://www.causeweb.org/cause/resources/library/r609#:~:text=The%20only%20statistics%20you%20can,Churchill%20(1874%20%2D%201965).\n\n\nGardner, Peter B. 2023. “Meet Shane Reese.” https://magazine.byu.edu/article/outlier/."
  },
  {
    "objectID": "gpa_analysis.html#introduction",
    "href": "gpa_analysis.html#introduction",
    "title": "2  Simple Linear Regression",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nThe hope of this analysis is to demonstrate that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to:\n\n\nGather inference to determine how GPA changes based on increase study time\n\n\nPredict what a student’s GPA would be based on study time\n\n\nData for this analysis can be found here"
  },
  {
    "objectID": "gpa_analysis.html#exploratory-data-analysis-eda",
    "href": "gpa_analysis.html#exploratory-data-analysis-eda",
    "title": "2  Simple Linear Regression",
    "section": "2.2 Exploratory Data Analysis (EDA)",
    "text": "2.2 Exploratory Data Analysis (EDA)\n\n2.2.1 Import data and libraries\nWe begin our analysis by importing the following libraries to create data visualizations as well as properly format and filter our data.\n\n#Import libraries\nimport opendatasets as od\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWe read in our data using pandas and will use the head function to check out the data.\n\n#Check dataframe\ndf.head()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n0\n4.00\n10.0\n\n\n1\n3.80\n25.0\n\n\n2\n3.93\n45.0\n\n\n3\n3.40\n10.0\n\n\n4\n3.20\n4.0\n\n\n\n\n\n\n\nThe head result indicates that the data has been read in correctly. In the next section, we will begin to clean the data.\n\n\n2.2.2 Data Cleaning\nWe will begin our data cleaning by getting some basic summary statistics from the dataframe.\n\n#Describe dataset\ndf.describe()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\ncount\n193.000000\n193.000000\n\n\nmean\n3.586166\n17.476684\n\n\nstd\n0.285482\n11.408980\n\n\nmin\n2.600000\n2.000000\n\n\n25%\n3.400000\n10.000000\n\n\n50%\n3.620000\n15.000000\n\n\n75%\n3.800000\n20.000000\n\n\nmax\n4.300000\n69.000000\n\n\n\n\n\n\n\nThe resulting dataframe shows that we have 193 rows of data where on average the gpa is 3.58 with a standard deviation of .28. Average study hours show 17.47 and standard deviation of 11.4, meaning we have a large spread in the distribution of study hours but not too much in gpa. From the summary statistics, we can easily see that the max value is invalid as the scale we are using for this analysis is 0-4. We will eliminate any rows that fall outside of this range.\n\n#Check datatypes\ndf.dtypes\n\ngpa            float64\nstudy_hours    float64\ndtype: object\n\n\nChecking the datatypes of each of the columns, we see that each is of type float. This confirms that all rows in each column are of the same datatype and, in theory, we do not need to clean up any corrupted data/poorly entered data.\n\n#Check for null values\ndf.isnull().sum()\n\ngpa            0\nstudy_hours    0\ndtype: int64\n\n\nIn addition, checking for nulls, we see there are none so we do not need to do anything with those. In the next code chunk, we will eliminate erroneous gpa entries.\n\n#Convert outlier GPA (erroneous entries) to 4.0\ndf = df.assign(gpa = lambda x: np.where(x.gpa &gt; 4, 4, x.gpa))\n\nIn the above chunk, we use the assign function to override the gpa column and change values that are above 4 to just 4 while leaving all others the same."
  },
  {
    "objectID": "gpa_analysis.html#data-viz",
    "href": "gpa_analysis.html#data-viz",
    "title": "2  Simple Linear Regression",
    "section": "2.3 Data Viz",
    "text": "2.3 Data Viz\nThe first visualization we will look at is comparing the distributions of gpa and study hours. We want to get a side by side look so we will use the subplots functionality from matplotlib. As well, we will plot the median of the two respective distributions to know where the 50% percentile lies. The following code produces Figure 2.1.\n\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\");\n\n#fig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\nFigure 2.1: Distributions of GPA and Study Hours\n\n\n\n\nFrom Figure 2.1, we see that GPA appears to be slightly left skewed with a couple of outliers to the left of the distribution. Study hours appears to be more right skewed with more outliers towards the right of the distribution. Median for GPA is 3.62 and 15 for study hours.\nSince we are working with two quantitative variables, we can use a scatterplot to view how linear their relationship (ie correlation) as well as view the where each gpa falls for a given amount of study hour. The below code produces Figure 2.2.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 2.2: Scatterplot of GPA by Study Hours\n\n\n\n\nFigure 2.2 shows that the linear relationship between study hours and GPA is not very strong. An exact score of .14 for the correlation indicates a weak linear relationship. Further more, like we already saw from the histograms, the data appears to be more grouped towards the beginning of the x axis (0-20ish hours).\nWe can attempt to adjust the distribution and correlation of the dataset by removing outliers. However, because the outlying dots appear to follow the trend of the data (more or less…) and we always like keeping as much data as possible, this is merely an optional step to fulfill our curiosity.\nWe will define an outlier as a point that falls outside of a box-plot range. Below is the function we will use to define and identify outliers.\n\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\nThe function calculates the IQR of the dataset and then returns a filtered dataframe based on whether a point passes the upper or lower bound of the box-plot. For example, below we see how the function identifies outliers using the GPA attribute.\n\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\nSimilarily, we can view the outliers for study hours below.\n\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n2\n3.930\n45.0\n\n\n7\n3.400\n40.0\n\n\n13\n3.830\n60.0\n\n\n51\n3.868\n40.0\n\n\n57\n3.125\n36.0\n\n\n77\n3.566\n40.0\n\n\n83\n3.850\n69.0\n\n\n89\n3.700\n45.0\n\n\n122\n3.750\n40.0\n\n\n125\n3.500\n49.0\n\n\n130\n3.825\n60.0\n\n\n135\n3.600\n40.0\n\n\n169\n3.830\n60.0\n\n\n\n\n\n\n\nFigure 2.3 below shows which points are labeled as outliers from our scatterplot.\n\n#Plot original scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\")\n\n#Show outliers by study hours (color red)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"study_hours\"), color='red')\n\n#Show outliers by gpa (color yellow)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"gpa\"), color='yellow');\n\n\n\n\nFigure 2.3: Scatterplot of GPA and Study Hours (highlighted outliers)\n\n\n\n\nAs already indicated, the red dots are outliers according to the “study hours” metric while yellow indicates outlier for “gpa”. While the red dots are technically “outliers”, they do sort of follow the general trend and direction of the data whereas the yellow dot does not. Let’s try removing the yellow dot to see how it changes the correlation of the data.\n\n#Remove outliers from dataset\ndf_filtered = df.query(\"gpa &gt; 2.6\")\n\nAfter removing the gpa outlier, the scatterplot and corresponding correlation are as follows.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df_filtered)\ncorr_coef = np.corrcoef(df_filtered.study_hours, df_filtered.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 2.4: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nWe see that removing the gpa outlier had a minimal effect on the correlation (actually slightly decreased) thus “weakening” the linear relationship of the variables. Perhaps we can try a non-linear transformation to aid our non-linear data.\nTo do this, we will perform a log transformation on both gpa and study hours (on our original dataset) and a polynomial transformation on study hours. Below is the result of our transformations.\n\n#Plot log transformed data\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.regplot(x='study_hours', y='gpa', data=np.log(df), ax=ax1)\nax1.set_xlabel(\"log(study_hours)\")\nax1.set_ylabel(\"log(gpa)\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax1.text(np.log(30), np.log(2.8), f\"Correlation: {corr_coef:.2f}\")\n\n#Plot polynomial transformed data\ndf_poly = df.assign(study_hours = lambda x: x.study_hours**2)\nsns.regplot(x='study_hours', y='gpa', data=df_poly, ax=ax2)\nax2.set_xlabel(\"study_hours^2\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax2.text(50**2, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 2.5: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nIn Figure 2.5, we see that our transformations really had not effect on the linear relationship. For the sake of getting on with the modeling, we will use our base dataset and interpret the model accordingly (barring that the other model assumptions hold)."
  },
  {
    "objectID": "gpa_analysis.html#classical-modeling",
    "href": "gpa_analysis.html#classical-modeling",
    "title": "2  Simple Linear Regression",
    "section": "2.4 Classical Modeling",
    "text": "2.4 Classical Modeling\nThis section will be our first attempt at modeling the data, using classical (frequentist) style approaches. Even though this is thebayesianbandit, I like providing frequentist approaches as well to show the differences and similarities between normal statistical modeling (frequentist) and bayesian modeling.\n\n2.4.1 Linear Regression Framework\nSince we are dealing with “linear” data, it would make sense to attempt to attempt to fit a linear model to the data. Therefore, we will be utilizing a simple linear regression (SLR) model to model the relationship between GPA and study hours (per week). Below is the equation we will use.\n\\[\ny = X\\beta + \\epsilon\n\\tag{2.1}\\] \\[\\epsilon \\sim N(\\mu, \\sigma^2)\\]\nEquation 2.1 models our response variable is equal to our matrix of explanatory variables (in this case, just one) multiplied (scaled) by a matrix of betas, plus errors (residuals) that are distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nIn linear algebra/matrix notation, it would look something like this.\n\\[\n\\begin{bmatrix}\ny_1 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1 \\\\\n... & ... \\\\\n1 & x_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n... \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\tag{2.2}\\]\nWe utilize ordinary least squares (OLS) to estimate our coefficients (\\(\\beta\\) values) by minimizing the residual sum of squares (RSS). Below are the equations we will use.\n\\[\nRSS = \\epsilon^2_1 + ... + \\epsilon^2_n\n\\tag{2.3}\\]\n\\[\n\\epsilon = y_i - \\hat{y_i}\n\\tag{2.4}\\]\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\tag{2.5}\\]\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\tag{2.6}\\]\nBy estimating our coefficients like this, we have a mathematical framework of estimating the line of best fit (the average line between all data points). This yields are final equation below.\n\\[\n\\hat{y} = X\\hat{\\beta}\n\\tag{2.7}\\]\nWhere \\(\\hat{y}\\) is our “predicted” value based off our inputs in the matrix of \\(X\\), scaled/multiplied by \\(\\hat{\\beta}\\), our estimated coefficients.\n\n\n2.4.2 Calculate beta values and interpret results\nNow that we have our framework by which we will fit our data, we can use Python libraries to perform the above calculations.\n\n#Import statsmod and fit SLR model\nimport statsmodels.api as sm\ny = df['gpa']\nX = pd.DataFrame({'intercept':np.ones(df.shape[0]), 'study_hours':df['study_hours']})\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n\n#Print summary from SLR model\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngpa\nR-squared:\n0.019\n\n\nModel:\nOLS\nAdj. R-squared:\n0.014\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.714\n\n\nDate:\nWed, 06 Sep 2023\nProb (F-statistic):\n0.0555\n\n\nTime:\n20:33:28\nLog-Likelihood:\n-27.443\n\n\nNo. Observations:\n193\nAIC:\n58.89\n\n\nDf Residuals:\n191\nBIC:\n65.41\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n3.5249\n0.037\n95.292\n0.000\n3.452\n3.598\n\n\nstudy_hours\n0.0034\n0.002\n1.927\n0.055\n-8.05e-05\n0.007\n\n\n\n\n\n\nOmnibus:\n10.671\nDurbin-Watson:\n2.385\n\n\nProb(Omnibus):\n0.005\nJarque-Bera (JB):\n11.481\n\n\nSkew:\n-0.595\nProb(JB):\n0.00321\n\n\nKurtosis:\n2.897\nCond. No.\n38.3\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFrom the summary, we have \\(\\beta_0\\) = 3.5249 and \\(\\beta_1\\) = .0034, meaning on average someone who studies 0 hours a week would see a GPA value of about 3.5. In addition, on average, for an increase of 1 hour in study time, we would see an increase of about .0034 in GPA. This isn’t too surprising considering the weak linear relationship we observed earlier in Figure 2.2. Essentially, if I were a student in this class, in order to change a whole letter grade (i.e. go from B to B+) and assuming I was right at B level (3.0), I would need to study 88 extra hours. Essentially, this model says that you’re stuck where you’re at.\nNevertheless, we can gain confidence in knowing a couple of things.\n\n\nThe data has a fairly weak linear relationship, so using a linear model to fit the data may not tell the most accurate story on how GPA is influenced by study hours.\n\n\nThe data could be inaccurate due to self-reporting\n\n\n\n\n2.4.3 Is study hours a statistically significant variable in determining GPA?\nTo determine the answer to the above question, we pose the following hypotheses.\n\\[\nH_0: \\beta_1 = 0\n\\] \\[\nH_a: \\beta_1 \\neq 0\n\\]\nThe aim of these hypotheses is to determine whether or not \\(\\beta_1\\) on average always has some kind of impact on GPA (whether it is an increase or decrease, no one knows in these things). We set our \\(\\alpha\\) = .05 to run our hypotheses.\nThe p-value we obtain from the summary print out indicates that p-value &gt; \\(\\alpha\\), so we fail to reject \\(H_0\\) and conclude that there isn’t sufficient evidence to reject the null. Essentially, we don’t have enough statistical evidence from the data to prove that study hours, on average, will never have a 0 impact on GPA.\nTo confirm this notion, we can view the corresponding 95% confidence interval for \\(\\beta_1\\) provided in the summary tab. The values of (-.000085, .007) indicate that 0 is included in the interval and therefore, the average change on GPA based on study hours can be 0.\n\n\n2.4.4 Check assumptions of linear regression model\nEven though the \\(\\beta_1\\) value proved to not be statistically significant (though very close), we should still verify that the assumptions of our linear model hold with this data set. The assumptions for a linear regression model are listed below.\n\n\nLinearity\n\n\nIndependence\n\n\nNormality\n\n\nEqual Variance\n\n\nWe already verified that there is a weak linear relationship, but still a linear relationship since correlation was not 0. Independence is verified by the fact that each observation in the study was independent of one another, thus making them i.i.d random variables.\nFor the normality assumption, we can plot a histogram of the residuals to check that the residuals are approximately normally distributed. Below is the graph.\n\n#Plot histogram of residuals\nplt.figure(figsize=(15,5))\nsns.histplot(results.resid);\n\n\n\n\nFigure 2.6: Histogram of resiudals\n\n\n\n\nAs we can see in Figure 2.6, the distribution looks fairly left skewed with a long left tail. For ease of this analysis/exercise, we will say this is “approximately” normal, but this would more than likely fail in the real world.\nTo verify equal variance, we will view a scatterplot of the fitted vs residual values to make sure that there appears to be equal variance (good randomness, no trends) in the graph.\n\n#Plot scatterplot of fitted vs residuals\nplt.figure(figsize=(15,5))\nsns.scatterplot(x=results.fittedvalues,y=results.resid);\n\n\n\n\nFigure 2.7: Scatterplot of fitted vs residuals\n\n\n\n\nIn Figure 2.7, there appears to be no significant trends in the data (though it does tend to begin to taper as we increase across the x-axis). However, since there is nothing obvious or glaring, we can verify this assumption.\n\n\n2.4.5 Do students spend on average 9 hours a week studying?\nFrom my past university experience, many professors have told me that for every 1 hour of lecture, should be 3 hours of study/homework time. Assuming this class was a 3 credit hour class, most students should be spending about 9 hours studying outside of class. To test this theory, we will use a t-test for the following hypotheses.\n\\[H_0: \\mu = 9\\] \\[H_a: \\mu \\neq 9\\]\n\n#Perform t-test to see if students on average spend 9 hours studying\nfrom scipy.stats import ttest_1samp\nttest_1samp(a=df['study_hours'], popmean=9)\n\nTtestResult(statistic=10.32185697364204, pvalue=3.767865100544389e-20, df=192)\n\n\nThe above result with a super small p-value (less than \\(\\alpha\\) = .05) indicates that we can reject the null hypothesis and conclude that students do not study an average of 9 hours for this class. To determine what the range of possible average values are, we will calculate a 95% confidence interval below.\n\n#Calculate 95% confidence interval\nttest_1samp(a=df['study_hours'], popmean=9).confidence_interval()\n\nConfidenceInterval(low=15.856880282374465, high=19.096487593273206)\n\n\nWe are 95% confident that the true average study time spent is between 15.85 to 19.09 hours. So, on average, students are spending a lot more time studying for this class than the hoped benchmark set by this university (assuming this university has set the same benchmark as my other ones)."
  },
  {
    "objectID": "gpa_analysis.html#bayesian-modeling",
    "href": "gpa_analysis.html#bayesian-modeling",
    "title": "2  Simple Linear Regression",
    "section": "2.5 Bayesian Modeling",
    "text": "2.5 Bayesian Modeling\nFinally, getting into the bayesian part of the analysis! As with our classical approach, we will layout the framework we will use for this section. We will be using the bayesian model (Bayes theorem) shown below.\n\\[P(H|\\theta) = \\frac{P(\\theta|H) P(H)}{P(\\theta)} \\tag{2.8}\\]\n\\[H = \\text{Our Hypothesis (prior)}\\] \\[\\theta = \\text{Our Data}\\]"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Centre, Churchill. n.d. https://www.causeweb.org/cause/resources/library/r609#:~:text=The%20only%20statistics%20you%20can,Churchill%20(1874%20%2D%201965).\n\n\nGardner, Peter B. 2023. “Meet Shane Reese.” https://magazine.byu.edu/article/outlier/."
  }
]